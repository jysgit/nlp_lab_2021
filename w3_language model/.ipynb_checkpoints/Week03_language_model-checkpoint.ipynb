{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03: Simple Language Model\n",
    "\n",
    "This week, we want you to create a Language Model (LM) to judge how common a sentence is.  \n",
    "More specificly, you need to <u>calculate the probability of a given setnence</u> showing in an article.  \n",
    "\n",
    "*No need to make your output exactly the same as ours. As long as it's *reasonable* (which means you can explain it), you get your points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Language Model?\n",
    "\n",
    "<b>A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length $m$, it assigns a probability $P(w_1, \\ldots ,w_m)$ to the whole sequence.</b>\n",
    "<i>-- from <a href=\"https://en.wikipedia.org/wiki/Language_model\">Wikipedia</a></i>  \n",
    "\n",
    "To put it simply, Language Model calculates the probability of a word, a sequence of words, or a sentence.  \n",
    "This can be useful in many NLP tasks, like machine translation, spelling checking, speech recognition, etc.  \n",
    "\n",
    "Consider the following two sentence  \n",
    "\n",
    "> (1) He is looking <font color=\"green\">*to*</font> a new job.  \n",
    "\n",
    "and \n",
    "\n",
    "> (2) He is looking <font color=\"green\">*for*</font> a new job.  \n",
    "\n",
    "\n",
    "We can see that there's a grammar error in sentence 1, for which the LM should generate a lower probability.  \n",
    "Hence, if your LM is well-established, you are able to judge which sentence is more correct (or more common) among a set of candidate sentences.  \n",
    "\n",
    "But how to do so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Step 1 - Ngram frequency\n",
    "\n",
    "First of all, we need to calculate 1- and 2-gram frequency from the corpus.  \n",
    "**Again**, yes. As we have said in the first class, word/phrase frequency plays an important role in NLP.\n",
    "\n",
    "You should be familiar with this procedure now, so let's skip the boring explanation.  \n",
    "<small>*Please refer to assignment 1 and 2 if you have any question.</small>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(os.path.join('big.txt'), 'r') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of The Adventures of Sherlock Holmes\\nby Sir Arthur Conan Doyle\\n(#15 in our series by Sir Arthur Conan Doyle)\\n\\nCopyright laws are changing all over the world. Be sure to check the\\ncopyright laws for your country before downloading or redistributing\\nthis or any other Project Gutenberg eBook.\\n\\nThis header should be the first thing seen when viewing this Project\\nGutenberg file.  Please do not remove it.  Do not change or edit the\\nheader without written permission.\\n\\nPlease read the \"legal small print,\" and other information about the\\neBook and Project Gutenberg at the bottom of this file.  Included is\\nimportant information about your specific rights and restrictions in\\nhow the file may be used.  You can also find out about how to make a\\ndonation to Project Gutenberg, and how to get involved.\\n\\n\\n**Welcome To The World of Free Plain Vanilla Electronic Texts**\\n\\n**eBooks Readable By Both Humans and By Computers, Since 1971**\\n\\n*****These eBooks Were Prepared By Thousands of Volunteers!*****\\n\\n\\nTitle: The Adventures of Sherlock Holmes\\n\\nAuthor: Sir Arthur Conan Doyle\\n\\nRelease Date: March, 1999  [EBook #1661]\\n[Most recently updated: November 29, 2002]\\n\\nEdition: 12\\n\\nLanguage: English\\n\\nCharacter set encoding: ASCII\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK, THE ADVENTURES OF SHERLOCK HOLMES ***\\n\\n\\n\\n\\n(Additional editing by Jose Menendez)\\n\\n\\n\\nTHE ADVENTURES OF\\nSHERLOCK HOLMES\\n\\nBY\\n\\nSIR ARTHUR CONAN DOYLE\\n\\nCONTENTS\\n\\nI.\\tA Scandal in Bohemia\\nII.\\tThe Red-Headed League\\nIII.\\tA Case of Identity\\nIV.\\tThe Boscombe Valley Mystery\\nV.\\tThe Five Orange Pips\\nVI.\\tThe Man with the Twisted Lip\\nVII.\\tThe Adventure of the Blue Carbuncle\\nVIII.\\tThe Adventure of the Speckled Band\\nIX.\\tThe Adventure of the Engineer\\'s Thumb\\nX.\\tThe Adventure of the Noble Bachelor\\nXI.\\tThe Adventure of the Beryl Coronet\\nXII.\\tThe Adventure of the Copper Beeches\\n\\n\\nADVENTURE  I.  A SCANDAL IN BOHEMIA\\n\\nI.\\n\\n\\nTo Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. He never spoke of the softer passions, save with a gibe and a sneer. They were admirable things for the observer--excellent for drawing the veil from men\\'s motives and actions. But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his. And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\\n\\nI had seen little of Holmes lately. My marriage had drifted us away from each other. My own complete happiness, and the home-centred interests which rise up around the man who first finds himself master of his own establishment, were sufficient to absorb all my attention, while Holmes, who loathed every form of society with his whole Bohemian soul, remained in our lodgings in Baker Street, buried among his old books, and alternating from week to week between cocaine and ambition, the drowsiness of the drug, and the fierce energy of his own keen nature. He was still, as ever, deeply attracted by the study of crime, and occupied his immense faculties and extraordinary powers of observation in following out those clues, and clearing up those mysteries which had been abandoned as hopeless by the official police. From time to time I heard some vague account of his doings: of his summons to Odessa in the case of the Trepoff murder, of his clearing up of the singular tragedy of the Atkinson brothers at Trincomalee, and finally of the mission which he had accomplished so delicately and successfully for the reigning family of Holland. Beyond these signs of his activity, however, which I merely shared with all the readers of the daily press, I knew little of my former friend and companion.\\n\\nOne night--it was on the twentieth of March, 1888--I was returning from a journey to a patient (for I had now returned to civil practice), when my way led me through Baker Street. As I passed the well-remembered door, which must always be associated in my mind with my wooing, and with the dark incidents of the Study in Scarlet, I was seized with a keen desire to see Holmes again, and to know how he was employing his extraordinary powers. His rooms were brilliantly lit, and, even as I looked up, I saw his tall, spare figure pass twice in a dark silhouette against the blind. He was pacing the room swiftly, eagerly, with his head sunk upon his chest '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = text.translate(str.maketrans('', '', string.punctuation+'\\t\\n\\'')).split(' ')\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def calculate_frequency(tokens):\n",
    "    frequency = collections.Counter(tokens)\n",
    "    return frequency\n",
    "\n",
    "def get_ngram(tokens, n=2):\n",
    "    ngram = []\n",
    "    for i in range(len(tokens)):\n",
    "        ngram.append(' '.join(tokens[i: i+n]))\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate 1- and 2-gram frequency with the given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram(text):\n",
    "    \"\"\" \n",
    "    [TODO] Get the 1-gram list from string\n",
    "    @param text: given string\n",
    "    @return: a list of token\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = text.translate(str.maketrans('', '', string.punctuation+'\\t\\n\\'')).split(' ')\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def bigram(text, n=2):\n",
    "    \"\"\" [TODO] Get the 2-gram list \"\"\"\n",
    "    tokens = unigram(text)\n",
    "    \n",
    "    ngram = []\n",
    "    for i in range(len(tokens)):\n",
    "        ngram.append(' '.join(tokens[i: i+n]))\n",
    "    return ngram\n",
    "\n",
    "def calculate_frequency(tokens):\n",
    "    frequency = collections.Counter(tokens)\n",
    "    frequency[' '] = 0\n",
    "    frequency[''] = 0\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_freq['book read']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Probability of a word\n",
    "\n",
    "#### Unigram  \n",
    "Probability of a word, or a *unigram*, is simple: its occurence devided by the total word count should do the trick.  \n",
    "$$P(w) = \\frac{Freq(w)}{N}$$  \n",
    ", where $N = $ count of all words.  \n",
    "But the unigram language model doesn't consider other context words, so we choose to use bigram model here.  \n",
    "\n",
    "#### Bigram  \n",
    "As to 2-gram probability, we would use the Maximum Likelihood Estimate (MLE) to calculate it.  \n",
    "\n",
    "$$P_{mle}(w_i|w_{i-1}) = \\frac{Freq(w_{i-1}, w_i)}{Freq(w_{i-1})}$$\n",
    "\n",
    "For example, if we have a corpus with \n",
    "\n",
    "> can you tell me about any good cantonese restaurants  \n",
    "> tell me about chez panisse  \n",
    "> can you give me a listing of the kinds of food that are available  \n",
    "\n",
    "Then the probability of *tell* preceding *me* is $P_{mle}(me|tell) = \\frac{2}{2} = 1$.  \n",
    "Similarly, the probability of *me* preceding *about* is $P_{mle}(about|me) = \\frac{2}{3}$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate the bigram probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_freq = calculate_frequency(unigram(corpus)) \n",
    "bi_freq = calculate_frequency(bigram(corpus)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_mle(word: str, pre_word: str = None):\n",
    "    \"\"\" [TODO] Return the P(word|pre_word), which should be a float \"\"\"\n",
    "    bigram = pre_word + ' ' + word\n",
    "    prob = bi_freq[bigram] / uni_freq[word]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0; \n",
      "p2: 0.021739130434782608\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('book', 'read')\n",
    "p2 = P_mle('books', 'read')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2` (expetced to be `0`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0; \n",
      "p2: 0.18823529411764706\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('books', 'a')\n",
    "p2 = P_mle('book', 'a')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2` (expetced to be `0`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0019298809906722419; \n",
      "p2: 0.1110320284697509\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('have', 'he')\n",
    "p2 = P_mle('has', 'he')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Sentence probability\n",
    "\n",
    "Now we have the probability of every word and bigram. But how should we combine them and get the probability of a sentence?  \n",
    "\n",
    "Recall the chain rule in probability: \n",
    "$$P(X_1, X_2, X_3, X_4) = P(X_1) \\cdot P(X_2|X_1) \\cdot P(X_3|X_1,X_2) \\cdot P(X_4|X_1, X_2, X_3)$$  \n",
    "And with Markove Assumption, we know that \n",
    "$$P(w_1, w_2, \\dots, w_n) \\approx P(w_2|w_1) \\cdot P(w_3|w_2) \\cdot \\dots \\cdot P(w_n|w_{n-1})$$\n",
    "\n",
    "For example,\n",
    "$$P(I\\:want\\:a\\:cake) \\approx P(want|I) \\cdot P(a|want) \\cdot P(cake|a)$$\n",
    "\n",
    "Since we have already built a method to calculate $P(w_{i}|w_{i-1})$, we can get the probability of a sentence through this equation.  \n",
    "Note that it's recommended to sum them under $log$-scale to prevent underflow, because gram probabilities are mostly some small floating points, \n",
    "$$log(p_1 \\cdot p_2 \\dots p_n) = log(p_1) + log(p_2) + \\dots + log(p_n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate the sentence probability. (Please do so under the `log` scale to prevent underflow)  \n",
    "<small>*Hint: `math`</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_mle(word: str, pre_word: str = None):\n",
    "    \"\"\" [TODO] Return the P(word|pre_word), which should be a float \"\"\"\n",
    "    bigram = pre_word + ' ' + word\n",
    "    prob = bi_freq[bigram] / uni_freq[word]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prob(sentence: str, P):\n",
    "    \"\"\" Calculate the probability of a given sentence with given P-model.\n",
    "    @param sentence: the given sentence\n",
    "    @param P: the probability model to use. (like your `P_mle` above)\n",
    "    @return: a float indicating the probability\n",
    "    \"\"\"\n",
    "    import math\n",
    "    words = unigram(sentence)\n",
    "    ... # [TODO] calculate the probability\n",
    "    sent_prob = 1\n",
    "    for i in range(1, len(words)):\n",
    "        bi_prob = math.log(P(words[i], words[i-1]))\n",
    "        sent_prob = sent_prob + bi_prob\n",
    "    return sent_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: -21.413883161072413; \n",
      "p2: -18.905720185325723\n"
     ]
    }
   ],
   "source": [
    "p1 = sentence_prob('He have to take the medicine.', P=P_mle)\n",
    "p2 = sentence_prob('He has to take the medicine.', P=P_mle)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks good so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this model heavily depends on how large your dataset is.   \n",
    "If you give it the word or gram not existing in the corpus, the probability of it will be $0$, which causes problems during the calculation (because you can't divide numbers by $0$).  \n",
    "\n",
    "Try this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(has|he) = 0.015240706394645594\n",
      "P(to|has) = 0.016219588271990017\n",
      "P(eat|to) = 0.0009386407091952025\n",
      "P(medicine|eat) = 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/97/_63hwfl500qf82z7lbrcvv2c0000gn/T/ipykernel_82282/3234235347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_prob_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"He has to eat medicine.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP_mle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/97/_63hwfl500qf82z7lbrcvv2c0000gn/T/ipykernel_82282/2946895493.py\u001b[0m in \u001b[0;36msentence_prob_log\u001b[0;34m(sentence, P)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'P({words[i+1]}|{words[i]}) = {p}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "sentence_prob(\"He has to eat medicine.\", P=P_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Now you get the problem.  \n",
    "Surely you can expand your dataset to decrease the number of unseen phrases, but it's impossible to cover all phrases in the world.  \n",
    "Here the smoothing technique comes to rescue!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - smoothing\n",
    "\n",
    "A naïve solution is to set the default frequency as `1`.  \n",
    "For example, if the original frequency is\n",
    "\n",
    "> hello: 2  \n",
    "> world: 1  \n",
    "> empty: 0  \n",
    "> null: 0  \n",
    "\n",
    ", after defaulting the frequency to 1, it would be\n",
    "\n",
    "> hello: 3  \n",
    "> world: 2  \n",
    "> empty: <font color=\"red\">**1**</font><br/>\n",
    "> null: <font color=\"red\">**1**</font><br/>\n",
    "\n",
    "\n",
    "And if the frequency is all added by 1, the probability of bigram will now be  \n",
    "$$ P_{add1} = \\frac{Freq(w_{i-1}, w_i)+1}{Freq(w_{i-1})+N}$$\n",
    ", where $N = count\\:of\\:all\\:tokens$ .  \n",
    "\n",
    "This is called the **Add-1 Estimation**, or also **Laplace Smoothing**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Implement the Laplace smoothing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_add1(word: str, pre_word: str = None):\n",
    "    \"\"\" [TODO] Return the P(word|pre_word), which should be a float \"\"\"\n",
    "    bigram = pre_word + ' ' + word\n",
    "    prob = (bi_freq[bigram]+1) / (uni_freq[word]+len(bi_freq.keys()))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 2.0447639728946087e-06; \n",
      "p2: 4.0895279457892175e-06\n"
     ]
    }
   ],
   "source": [
    "p1 = P_add1('medicine', 'eat')\n",
    "p2 = P_add1('medicine', 'take')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 8.963472951375849e-07; \n",
      "p2: 1.7917919801182761e-06\n"
     ]
    }
   ],
   "source": [
    "p1 = P_add1('medicine', 'eat')\n",
    "p2 = P_add1('medicine', 'take')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> `p1` shouldn't throw any error, and `p1` should be **smaller** than `p2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Add-1 estimation is a really naive method and doesn't always produce good results.  \n",
    "There are better smooting techiniques like <a href=\"https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation\">Good-Turing Estimation</a> or <a href=\"https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing\">Kneser-Ney Smoothing</a>. Feel free to give it a try!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your LM\n",
    "\n",
    "Seems that you've done a great job! Let's give it some tests to see if it works as expected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He have to take the medicine. -48.05239652944902\n",
      "He has to eat the medicine. -52.6395763264315\n",
      "He has to take the medicine. -46.44266986776041\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "  'He have to take the medicine.',\n",
    "  'He has to eat the medicine.',\n",
    "  'He has to take the medicine.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "    print(sent, sentence_prob(sent, P=P_add1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> The last sentence should return the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is looking to a job. -48.93939664757258\n",
      "He is looking for a job. -48.06187722134649\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "  'He is looking to a job.',\n",
    "  'He is looking for a job.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "  print(sent, sentence_prob(sent, P=P_add1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> The second sentence should return a higher probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think about it\n",
    "Think about the following questions. **TA will ask your answers during the demo.**  \n",
    "\n",
    "1. What would happen if you use MLE Estimation to compare two sentences with different lengthes?  \n",
    "   Examples. (1) *He school nice and happy.* and (2) *He went to school yesterday and had a nice time there.*  \n",
    "\n",
    "2. According to Q1, Do you think MLE model is fair? If so, why? If not, how could you improve this? (Just think about it; no need to actually implement it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulation!** You've successfully built your own language model.  \n",
    "You should have caught a glimpse of LM's power despite its plainness.  \n",
    "Feel free to play around with your LM and improve it, like  \n",
    "1. using a larger dataset, \n",
    "2. applying better smoothing technique, or \n",
    "3. considering longer gram dependency (3- to 5-gram are resonable length).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TA's note\n",
    "\n",
    "\n",
    "Remember to <b><a href=\"https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit?usp=sharing\">make an appoiment with TA</a> to demo/explain your implementation <u>before <font color=\"red\">10/7 15:30</font></u></b> .  \n",
    "You should also save your file as {student_id}.ipynb and submit it to <a href=\"https://eeclass.nthu.edu.tw/course/homework/2384\">eeclass</a> .  \n",
    "**Late submission is not allowed.**  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e56d558c252a4887cb9bc110bf182e2aa9946109639baab21a0ce4014d1d01c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
